{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 27\n"
     ]
    }
   ],
   "source": [
    "from data_loading.animal_keypoints_dataset import AnimalKeypointsDataset\n",
    "from utils.transforms import RandomRotation, RandomFlip, RandomRatioCrop\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from models.conv_pose_machines import ConvolutionalPoseMachines\n",
    "from utils.losses import MSECELoss, HMapsMSELoss\n",
    "from utils.set_random_seed import set_random_seed, SEED\n",
    "import torch\n",
    "import os\n",
    "# %cd ../../\n",
    "from pose_estimation.cats.train import train\n",
    "from utils.model_saver import ModelSaver\n",
    "from utils.logger import Logger\n",
    "\n",
    "# %cd ./pose_estimation/cats/\n",
    "\n",
    "set_random_seed(SEED)\n",
    "\n",
    "INIT_WEIGHT_PATH = '../../models/weights/ConvolutionalPoseMachines_4_stages/HMapsMSELoss/Adam_lr_1e-05_betas_(0o9_0o999)_eps_1e-08/ConvolutionalPoseMachines_E899_B5.pth'\n",
    "ALPHA = 0.00001\n",
    "IMAGE_SIZE = (368, 368)\n",
    "EPOCHS = 900\n",
    "TRAIN_BATCH_SIZE = 5\n",
    "TEST_BATCH_SIZE = 5\n",
    "# LOG_STEP = 30\n",
    "N_SUBSTAGES = 3\n",
    "SAVE_MODEL_STEP = 90\n",
    "START_EPOCH = 900\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "all_tform = transforms.Compose([\n",
    "    RandomFlip(0.5, 0.5),\n",
    "    RandomRatioCrop(0.1, 0.1, 0.9, 0.9),\n",
    "    RandomRotation((-30, 30)),\n",
    "])\n",
    "\n",
    "img_tform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "data_train = AnimalKeypointsDataset(\n",
    "    json_file_path='../../dataset/cats/train/keypoints_annotations.json',\n",
    "    image_dir='../../dataset/cats/train/labeled/',\n",
    "    transform={'all': all_tform,\n",
    "               'image': img_tform,\n",
    "               'keypoints': transforms.ToTensor()},\n",
    "    heatmap=True)\n",
    "data_train_loader = DataLoader(data_train, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "data_test = AnimalKeypointsDataset(\n",
    "    json_file_path='../../dataset/cats/test/keypoints_annotations.json',\n",
    "    image_dir='../../dataset/cats/test/labeled/',\n",
    "    transform={'all': all_tform,\n",
    "               'image': img_tform,\n",
    "               'keypoints': transforms.ToTensor()},\n",
    "    heatmap=True)\n",
    "data_test_loader = DataLoader(data_test, batch_size=TEST_BATCH_SIZE, shuffle=True, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's weights will be saved to: ../../models/weights/ConvolutionalPoseMachines/HMapsMSELoss/Adam_lr_1e-05_betas_(0o9_0o999)_eps_1e-08/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mtechnumber\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.13.9 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.7"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/tehnik/coding/pycharm/pytorch_course_animal_pose_estimation/pose_estimation/cats/wandb/run-20230207_001111-3m6ul7uc</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/technumber/animal_pose_estimation/runs/3m6ul7uc\" target=\"_blank\">glad-fire-1</a></strong> to <a href=\"https://wandb.ai/technumber/animal_pose_estimation\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3fed7f4c098f4e02bdbf10539afa46d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>test/loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▂▁▂▂▃▄▃▄▆█▆▄</td></tr><tr><td>test/min_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▇▆▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▂▃▂▂▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>899</td></tr><tr><td>test/loss</td><td>69996968.03074</td></tr><tr><td>test/min_loss</td><td>1.9164</td></tr><tr><td>train/loss</td><td>1.53058</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced <strong style=\"color:#cdcd00\">glad-fire-1</strong>: <a href=\"https://wandb.ai/technumber/animal_pose_estimation/runs/3m6ul7uc\" target=\"_blank\">https://wandb.ai/technumber/animal_pose_estimation/runs/3m6ul7uc</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20230207_001111-3m6ul7uc/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ConvolutionalPoseMachines(\n",
    "    n_keypoints=16,\n",
    "    n_substages=N_SUBSTAGES,\n",
    "    n_base_ch=80,\n",
    "    img_feat_ch=20\n",
    ").to(device)\n",
    "if os.path.isfile(INIT_WEIGHT_PATH):\n",
    "    model.load_state_dict(torch.load(INIT_WEIGHT_PATH))\n",
    "else:\n",
    "    print(\"Weights not found.\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=ALPHA)\n",
    "loss = HMapsMSELoss().to(device)\n",
    "\n",
    "model_saver = ModelSaver(\n",
    "    model,\n",
    "    TRAIN_BATCH_SIZE,\n",
    "    save_freq=SAVE_MODEL_STEP,\n",
    "    start_epoch=START_EPOCH,\n",
    "    loss=loss,\n",
    "    optimizer=optimizer\n",
    ")\n",
    "\n",
    "logger = Logger(\n",
    "    model,\n",
    "    IMAGE_SIZE,\n",
    "    EPOCHS,\n",
    "    TRAIN_BATCH_SIZE,\n",
    "    loss=loss,\n",
    "    optimizer=optimizer,\n",
    "    n_substages=N_SUBSTAGES,\n",
    "    dataset='cats',\n",
    "    start_epoch=START_EPOCH\n",
    ")\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    data_train=data_train_loader,\n",
    "    data_test=data_test_loader,\n",
    "    loss=loss,\n",
    "    optimizer=optimizer,\n",
    "    epochs=EPOCHS,\n",
    "    logger=logger,\n",
    "    model_saver=model_saver,\n",
    "    device=device\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitpytorchcondaf04cb2303bb94659b54446e023c3cb62",
   "display_name": "Python 3.8.2 64-bit ('pytorch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
