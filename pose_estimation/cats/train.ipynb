{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 27\n"
     ]
    }
   ],
   "source": [
    "from data_loading.animal_keypoints_dataset import AnimalKeypointsDataset\n",
    "from utils.transforms import RandomRotation, RandomFlip, RandomRatioCrop\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from models.conv_pose_machines import ConvolutionalPoseMachines\n",
    "from utils.losses import MSECELoss, HMapsMSELoss\n",
    "from utils.set_random_seed import set_random_seed, SEED\n",
    "import torch\n",
    "import os\n",
    "# %cd ../../\n",
    "from pose_estimation.cats.train import train\n",
    "from utils.model_saver import ModelSaver\n",
    "# %cd ./pose_estimation/cats/\n",
    "\n",
    "set_random_seed(SEED)\n",
    "\n",
    "INIT_WEIGHT_PATH = '../../models/weights/ConvolutionalPoseMachines_4_stages/HMapsMSELoss/Adam_lr_1e-05_betas_(0o9_0o999)_eps_1e-08/ConvolutionalPoseMachines_E899_B5.pth'\n",
    "ALPHA = 0.00001\n",
    "IMAGE_SIZE = (368, 368)\n",
    "EPOCHS = 900\n",
    "TRAIN_BATCH_SIZE = 5\n",
    "TEST_BATCH_SIZE = 5\n",
    "LOG_STEP = 30\n",
    "SAVE_MODEL_STEP = 90\n",
    "START_EPOCH = 900\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "all_tform = transforms.Compose([\n",
    "    RandomFlip(0.5, 0.5),\n",
    "    RandomRatioCrop(0.1, 0.1, 0.9, 0.9),\n",
    "    RandomRotation((-30, 30)),\n",
    "])\n",
    "\n",
    "img_tform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "data_train = AnimalKeypointsDataset(\n",
    "    json_file_path='../../dataset/cats/train/keypoints_annotations.json',\n",
    "    image_dir='../../dataset/cats/train/labeled/',\n",
    "    transform={'all': all_tform,\n",
    "               'image': img_tform,\n",
    "               'keypoints': transforms.ToTensor()},\n",
    "    heatmap=True)\n",
    "data_train_loader = DataLoader(data_train, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "data_test = AnimalKeypointsDataset(\n",
    "    json_file_path='../../dataset/cats/test/keypoints_annotations.json',\n",
    "    image_dir='../../dataset/cats/test/labeled/',\n",
    "    transform={'all': all_tform,\n",
    "               'image': img_tform,\n",
    "               'keypoints': transforms.ToTensor()},\n",
    "    heatmap=True)\n",
    "data_test_loader = DataLoader(data_test, batch_size=TEST_BATCH_SIZE, shuffle=True, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's weights will be saved to: ../../models/weights/ConvolutionalPoseMachines/HMapsMSELoss/Adam_lr_1e-05_betas_(0o9_0o999)_eps_1e-08/\n",
      "Train loss: 1.8321841273988997, Epoch: 0\n",
      "Test loss: 1.9634372472763062, Epoch: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 22\u001B[0m\n\u001B[1;32m     13\u001B[0m loss \u001B[38;5;241m=\u001B[39m HMapsMSELoss()\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     15\u001B[0m model_saver \u001B[38;5;241m=\u001B[39m ModelSaver(model,\n\u001B[1;32m     16\u001B[0m                          TRAIN_BATCH_SIZE,\n\u001B[1;32m     17\u001B[0m                          save_freq\u001B[38;5;241m=\u001B[39mSAVE_MODEL_STEP,\n\u001B[1;32m     18\u001B[0m                          start_epoch\u001B[38;5;241m=\u001B[39mSTART_EPOCH,\n\u001B[1;32m     19\u001B[0m                          loss\u001B[38;5;241m=\u001B[39mloss,\n\u001B[1;32m     20\u001B[0m                          optimizer\u001B[38;5;241m=\u001B[39moptimizer)\n\u001B[0;32m---> 22\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_train_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_test\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_test_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m    \u001B[49m\u001B[43mloss\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mEPOCHS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_saver\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_saver\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogging_step\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mLOG_STEP\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/coding/pycharm/pytorch_course_animal_pose_estimation/pose_estimation/cats/train.py:42\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, data_train, data_test, loss, optimizer, epochs, model_saver, logging_step, device)\u001B[0m\n\u001B[1;32m     39\u001B[0m train_img \u001B[38;5;241m=\u001B[39m batch_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     40\u001B[0m train_hmap \u001B[38;5;241m=\u001B[39m batch_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mheatmap\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 42\u001B[0m pred_hmaps \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_img\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m loss_value \u001B[38;5;241m=\u001B[39m loss(pred_hmaps, train_hmap\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     45\u001B[0m batch_train_loss_list\u001B[38;5;241m.\u001B[39mappend(loss_value\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_pytorch_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/coding/pycharm/pytorch_course_animal_pose_estimation/models/conv_pose_machines.py:117\u001B[0m, in \u001B[0;36mConvolutionalPoseMachines.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    114\u001B[0m outputs \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minit_stage(x)]\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sub_stage \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msubsequent_stages_list:\n\u001B[0;32m--> 117\u001B[0m     outputs\u001B[38;5;241m.\u001B[39mappend(\u001B[43msub_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimg_ref\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mstack(outputs, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_pytorch_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/coding/pycharm/pytorch_course_animal_pose_estimation/models/conv_pose_machines.py:84\u001B[0m, in \u001B[0;36mSubsequentStage.forward\u001B[0;34m(self, x, img_ref)\u001B[0m\n\u001B[1;32m     82\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((x, img_ref), \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks:\n\u001B[0;32m---> 84\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_pytorch_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/coding/pycharm/pytorch_course_animal_pose_estimation/models/conv_pose_machines.py:25\u001B[0m, in \u001B[0;36mConv2dBlock.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m---> 25\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mact:\n\u001B[1;32m     27\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mact(x)\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_pytorch_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_pytorch_project/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_pytorch_project/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = ConvolutionalPoseMachines(\n",
    "        n_keypoints=16,\n",
    "        n_substages=3,\n",
    "        n_base_ch=80,\n",
    "        img_feat_ch=20\n",
    "    )\n",
    "model.to(device)\n",
    "if os.path.isfile(INIT_WEIGHT_PATH):\n",
    "    model.load_state_dict(torch.load(INIT_WEIGHT_PATH))\n",
    "else:\n",
    "    print(\"Weights not found.\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=ALPHA)\n",
    "loss = HMapsMSELoss().to(device)\n",
    "\n",
    "model_saver = ModelSaver(model,\n",
    "                         TRAIN_BATCH_SIZE,\n",
    "                         save_freq=SAVE_MODEL_STEP,\n",
    "                         start_epoch=START_EPOCH,\n",
    "                         loss=loss,\n",
    "                         optimizer=optimizer)\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    data_train=data_train_loader,\n",
    "    data_test=data_test_loader,\n",
    "    loss=loss,\n",
    "    optimizer=optimizer,\n",
    "    epochs=EPOCHS,\n",
    "    model_saver=model_saver,\n",
    "    logging_step=LOG_STEP,\n",
    "    device=device\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitpytorchcondaf04cb2303bb94659b54446e023c3cb62",
   "display_name": "Python 3.8.2 64-bit ('pytorch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
